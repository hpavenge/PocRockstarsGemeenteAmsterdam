Ollama - snel een llm antwoord via hhtp call
Semantic Kernel toegevoegd - zie uitleg onderstaand
fake mcp python server toegevoegd - zie uitleg onderstaand


Wat is Semantic Kernel (SK)?
Semantic Kernel is een .NET-library van Microsoft die fungeert als een AI â€œorchestratorâ€.
In plaats van zelf overal HTTP-calls te doen, geeft SK je:

Gemakkelijk model wisselen â†’ Azure OpenAI, Ollama, OpenAI, HuggingFaceâ€¦ met Ã©Ã©n regel aanpassing.

Prompt templates â†’ prompts netjes beheren en hergebruiken.

Function Calling â†’ AI kan automatisch functies (zoals jouw MCP-tools) aanroepen op basis van een vraag.

Memory â†’ context/gesprekken bewaren en hergebruiken.

Plugins â†’ bundels met AI-functies en externe tools.

Waarom in jouw POC handig
Omdat we straks meerdere dingen samen willen laten werken:

LLM (Ollama, later misschien Amsterdam-LLM of Azure).

Externe tools (via MCP).

Documenten opvragen (RAG).

Zonder SK moet jij zelf:

Prompt logic schrijven.

Bedenken wanneer welke tool wordt aangeroepen.

Context handmatig meenemen in het gesprek.

Met SK krijg je een framework dat dat netjes organiseert. Je kunt dus sneller schakelen, en later ook makkelijk uitbreiden.

Waarom semantic kernel ? 
 Waarom dit nu fijn is:

Als je straks een MCP-tool toevoegt, kan SK de keuze â€œwelke functie moet ik nu gebruikenâ€ zelf maken.

Je kunt ook makkelijk een â€œchainâ€ van prompts maken (bijvoorbeeld: eerst samenvatten, dan vertalen).

Wisselen van Ollama â†’ Azure LLM is Ã©Ã©n regel aanpassen.

MCP
MCP = een universele manier om AI-apps en externe functies/datasets aan elkaar te koppelen.
Je hoeft tools maar Ã©Ã©n keer te schrijven, en daarna kunnen meerdere AI-clients ze gebruiken zonder extra werk.

ğŸ”§ Hoe werkt dat in de praktijk?
Een MCP Server:

Is een proces (bijv. Python-script, Node-app, .NET-app) dat Ã©Ã©n of meerdere tools aanbiedt.

Houdt zich aan het MCP-protocol (bepaalt hoe functies worden beschreven en aangeroepen).

Kan van alles doen: data ophalen, berekeningen maken, externe APIâ€™s aanroepen, of documenten doorzoeken.

Een MCP Client:

Is de kant die de AI-app runt (bijv. jouw .NET Semantic Kernel-app).

Vraagt de MCP-server: â€œWelke tools heb je?â€

Stuurt inputs naar die tools en krijgt outputs terug.

Combineert dit met de LLM-output.